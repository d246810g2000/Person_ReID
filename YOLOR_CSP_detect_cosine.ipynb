{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d190132d",
   "metadata": {
    "id": "UL_9sBkuYsBz"
   },
   "source": [
    "# YOLOR + ReID 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f85ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('yolor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816384eb",
   "metadata": {
    "id": "IeXiY6goZNjE"
   },
   "source": [
    "## 匯入所需套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39b4c1b",
   "metadata": {
    "id": "a91de30c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.google_utils import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import (\n",
    "    check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, strip_optimizer)\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "\n",
    "from models.models import *\n",
    "from utils.datasets import *\n",
    "from utils.general import *\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349a2a91",
   "metadata": {
    "id": "811cd6d5"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69246e7",
   "metadata": {
    "id": "89uLk_rVZsh3"
   },
   "source": [
    "## 參數設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedc766a",
   "metadata": {
    "id": "5a4cafe9"
   },
   "outputs": [],
   "source": [
    "weights = 'weights/yolor_csp.pt'\n",
    "source = 'person_reid_datasets/test/'\n",
    "sub = 'person_reid_datasets/sample_submission.csv'\n",
    "cfg = 'cfg/yolor_csp_pedestrian.cfg'\n",
    "data = 'data/pedestrian.yaml'\n",
    "imgsz = 640\n",
    "conf_thres = 0.25\n",
    "iou_thres = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a8f61",
   "metadata": {
    "id": "ULXI0iSRZaJD"
   },
   "source": [
    "## 將訓練好的偵測模型權重和 ReID 模型權重讀入到各自的模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfc5f5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26c6a5ae",
    "outputId": "134ef9a3-30f9-465f-c1d5-df0411900942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Darknet(cfg, imgsz).to(device).eval()\n",
    "model.load_state_dict(torch.load(weights, map_location=device)['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5740f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Defines the new fc layer and classification layer\n",
    "# |--Linear--|--bn--|--relu--|--Linear--|\n",
    "class ClassBlock(nn.Module):\n",
    "    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, linear=512, return_f = False):\n",
    "        super(ClassBlock, self).__init__()\n",
    "        self.return_f = return_f\n",
    "        add_block = []\n",
    "        if linear>0:\n",
    "            add_block += [nn.Linear(input_dim, linear)]\n",
    "        else:\n",
    "            linear = input_dim\n",
    "        if bnorm:\n",
    "            add_block += [nn.BatchNorm1d(linear)]\n",
    "        if relu:\n",
    "            add_block += [nn.LeakyReLU(0.1)]\n",
    "        if droprate>0:\n",
    "            add_block += [nn.Dropout(p=droprate)]\n",
    "        add_block = nn.Sequential(*add_block)\n",
    "        \n",
    "        classifier = []\n",
    "        classifier += [nn.Linear(linear, class_num)]\n",
    "        classifier = nn.Sequential(*classifier)\n",
    "\n",
    "        self.add_block = add_block\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x):\n",
    "        x = self.add_block(x)\n",
    "        if self.return_f:\n",
    "            f = x\n",
    "            x = self.classifier(x)\n",
    "            return [x,f]\n",
    "        else:\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "        \n",
    "class ft_net(nn.Module):\n",
    "    def __init__(self, class_num=100, droprate=0.5, circle=False, linear_num=512):\n",
    "        super(ft_net, self).__init__()\n",
    "        model_ft = models.resnet50(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(model_ft.children())[:-1])\n",
    "        self.classifier = ClassBlock(2048, class_num, droprate, linear=linear_num, return_f = circle)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), x.size(1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "reid_model = torch.load('reid_model_cosine.pth', map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c4f35",
   "metadata": {
    "id": "l1kAub6baCpD"
   },
   "source": [
    "## 預測影像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7bdfe01",
   "metadata": {
    "id": "JgM71kIdaNl_"
   },
   "outputs": [],
   "source": [
    "dataset = LoadImages(source, img_size=imgsz, auto_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17af5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['1',\n",
    " '111',\n",
    " '112',\n",
    " '116',\n",
    " '117',\n",
    " '122',\n",
    " '123',\n",
    " '124',\n",
    " '125',\n",
    " '126',\n",
    " '132',\n",
    " '133',\n",
    " '135',\n",
    " '148',\n",
    " '15',\n",
    " '155',\n",
    " '162',\n",
    " '163',\n",
    " '166',\n",
    " '173',\n",
    " '174',\n",
    " '175',\n",
    " '181',\n",
    " '182',\n",
    " '183',\n",
    " '184',\n",
    " '185',\n",
    " '188',\n",
    " '192',\n",
    " '193',\n",
    " '197',\n",
    " '205',\n",
    " '216',\n",
    " '221',\n",
    " '222',\n",
    " '225',\n",
    " '228',\n",
    " '230',\n",
    " '231',\n",
    " '232',\n",
    " '233',\n",
    " '235',\n",
    " '265',\n",
    " '273',\n",
    " '285',\n",
    " '296',\n",
    " '3',\n",
    " '302',\n",
    " '338',\n",
    " '339',\n",
    " '341',\n",
    " '344',\n",
    " '359',\n",
    " '516',\n",
    " '517',\n",
    " '543',\n",
    " '544',\n",
    " '545',\n",
    " '55',\n",
    " '607',\n",
    " '61',\n",
    " '618',\n",
    " '619',\n",
    " '621',\n",
    " '622',\n",
    " '623',\n",
    " '624',\n",
    " '626',\n",
    " '63',\n",
    " '652',\n",
    " '653',\n",
    " '655',\n",
    " '656',\n",
    " '659',\n",
    " '660',\n",
    " '662',\n",
    " '663',\n",
    " '664',\n",
    " '705',\n",
    " '712',\n",
    " '713',\n",
    " '714',\n",
    " '734',\n",
    " '735',\n",
    " '736',\n",
    " '74',\n",
    " '75',\n",
    " '754',\n",
    " '835',\n",
    " '84',\n",
    " '857',\n",
    " '87',\n",
    " '88',\n",
    " '880',\n",
    " '881',\n",
    " '882',\n",
    " '891',\n",
    " '894',\n",
    " '90',\n",
    " '98']\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(class_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6d88c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05d01f15",
    "outputId": "d18393dc-9c08-465e-8b20-ba1ee724bfcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_01.jpg: 384x640 4\n",
      "image 2/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_02.jpg: 384x640 4\n",
      "image 3/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_03.jpg: 384x640 4\n",
      "image 4/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_04.jpg: 384x640 4\n",
      "image 5/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_05.jpg: 384x640 2\n",
      "image 6/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_06.jpg: 384x640 4\n",
      "image 7/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_07.jpg: 384x640 4\n",
      "image 8/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_08.jpg: 384x640 3\n",
      "image 9/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_09.jpg: 384x640 8\n",
      "image 10/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_10.jpg: 384x640 3\n",
      "image 11/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_11.jpg: 384x640 4\n",
      "image 12/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_12.jpg: 384x640 3\n",
      "image 13/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_13.jpg: 384x640 2\n",
      "image 14/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_14.jpg: 384x640 1\n",
      "image 15/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_15.jpg: 384x640 3\n",
      "image 16/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_16.jpg: 384x640 1\n",
      "image 17/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_17.jpg: 384x640 3\n",
      "image 18/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_18.jpg: 384x640 4\n",
      "image 19/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_19.jpg: 384x640 3\n",
      "image 20/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_20.jpg: 384x640 4\n",
      "image 21/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_21.jpg: 384x640 3\n",
      "image 22/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_22.jpg: 384x640 1\n",
      "image 23/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_23.jpg: 384x640 4\n",
      "image 24/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_24.jpg: 384x640 1\n",
      "image 25/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_25.jpg: 384x640 1\n",
      "image 26/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_26.jpg: 384x640 2\n",
      "image 27/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_27.jpg: 384x640 2\n",
      "image 28/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_28.jpg: 384x640 4\n",
      "image 29/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_29.jpg: 384x640 1\n",
      "image 30/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_30.jpg: 384x640 3\n",
      "image 31/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_31.jpg: 384x640 1\n",
      "image 32/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_32.jpg: 384x640 1\n",
      "image 33/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_33.jpg: 384x640 3\n",
      "image 34/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_34.jpg: 384x640 5\n",
      "image 35/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_35.jpg: 384x640 3\n",
      "image 36/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_36.jpg: 384x640 2\n",
      "image 37/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_37.jpg: 384x640 2\n",
      "image 38/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_38.jpg: 384x640 3\n",
      "image 39/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_39.jpg: 384x640 4\n",
      "image 40/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_40.jpg: 384x640 2\n",
      "image 41/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_41.jpg: 384x640 1\n",
      "image 42/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_42.jpg: 384x640 7\n",
      "image 43/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_43.jpg: 384x640 3\n",
      "image 44/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_44.jpg: 384x640 5\n",
      "image 45/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_45.jpg: 384x640 4\n",
      "image 46/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_46.jpg: 384x640 1\n",
      "image 47/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_47.jpg: 384x640 8\n",
      "image 48/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_48.jpg: 384x640 6\n",
      "image 49/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_49.jpg: 384x640 6\n",
      "image 50/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_50.jpg: 384x640 1\n",
      "image 51/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_51.jpg: 384x640 2\n",
      "image 52/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_52.jpg: 384x640 2\n",
      "image 53/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_53.jpg: 384x640 1\n",
      "image 54/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_54.jpg: 384x640 2\n",
      "image 55/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_55.jpg: 384x640 1\n",
      "image 56/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_56.jpg: 384x640 3\n",
      "image 57/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_57.jpg: 384x640 2\n",
      "image 58/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_58.jpg: 384x640 3\n",
      "image 59/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_59.jpg: 384x640 3\n",
      "image 60/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_60.jpg: 384x640 8\n",
      "image 61/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_61.jpg: 384x640 1\n",
      "image 62/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_62.jpg: 384x640 5\n",
      "image 63/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_63.jpg: 384x640 2\n",
      "image 64/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_64.jpg: 384x640 3\n",
      "image 65/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_65.jpg: 384x640 3\n",
      "image 66/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_66.jpg: 384x640 2\n",
      "image 67/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_67.jpg: 384x640 3\n",
      "image 68/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_68.jpg: 384x640 1\n",
      "image 69/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_69.jpg: 384x640 4\n",
      "image 70/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_70.jpg: 384x640 4\n",
      "image 71/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_71.jpg: 384x640 2\n",
      "image 72/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_72.jpg: 384x640 4\n",
      "image 73/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_01.jpg: 384x640 3\n",
      "image 74/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_02.jpg: 384x640 2\n",
      "image 75/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_03.jpg: 384x640 2\n",
      "image 76/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_04.jpg: 384x640 2\n",
      "image 77/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_05.jpg: 384x640 1\n",
      "image 78/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_06.jpg: 384x640 3\n",
      "image 79/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_07.jpg: 384x640 2\n",
      "image 80/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_08.jpg: 384x640 2\n",
      "image 81/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_09.jpg: 384x640 2\n",
      "image 82/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_10.jpg: 384x640 4\n",
      "image 83/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_11.jpg: 384x640 1\n",
      "image 84/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_12.jpg: 384x640 2\n",
      "image 85/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_13.jpg: 384x640 1\n",
      "image 86/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_14.jpg: 384x640 1\n",
      "image 87/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_15.jpg: 384x640 1\n",
      "image 88/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_16.jpg: 384x640 1\n",
      "image 89/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_17.jpg: 384x640 4\n",
      "image 90/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_18.jpg: 384x640 4\n",
      "image 91/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_19.jpg: 384x640 1\n",
      "image 92/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_20.jpg: 384x640 1\n",
      "image 93/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_21.jpg: 384x640 1\n",
      "image 94/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_22.jpg: 384x640 2\n",
      "image 95/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_23.jpg: 384x640 2\n",
      "image 96/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_24.jpg: 384x640 3\n",
      "image 97/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_25.jpg: 384x640 2\n",
      "image 98/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_26.jpg: 384x640 1\n",
      "image 99/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_27.jpg: 384x640 2\n",
      "image 100/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_28.jpg: 384x640 1\n",
      "image 101/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_29.jpg: 384x640 1\n",
      "image 102/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_30.jpg: 384x640 2\n",
      "image 103/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_31.jpg: 384x640 4\n",
      "image 104/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_32.jpg: 384x640 2\n",
      "image 105/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_33.jpg: 384x640 1\n",
      "image 106/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_34.jpg: 384x640 3\n",
      "image 107/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_01.jpg: 384x640 4\n",
      "image 108/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_02.jpg: 384x640 4\n",
      "image 109/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_03.jpg: 384x640 1\n",
      "image 110/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_04.jpg: 384x640 2\n",
      "image 111/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_05.jpg: 384x640 3\n",
      "image 112/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_06.jpg: 384x640 2\n",
      "image 113/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_07.jpg: 384x640 4\n",
      "image 114/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_08.jpg: 384x640 3\n",
      "image 115/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_09.jpg: 384x640 3\n",
      "image 116/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_10.jpg: 384x640 4\n",
      "image 117/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_11.jpg: 384x640 2\n",
      "image 118/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_12.jpg: 384x640 5\n",
      "image 119/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_13.jpg: 384x640 4\n",
      "image 120/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_14.jpg: 384x640 2\n",
      "image 121/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_15.jpg: 384x640 2\n",
      "image 122/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_16.jpg: 384x640 3\n",
      "image 123/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_17.jpg: 384x640 4\n",
      "image 124/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_18.jpg: 384x640 3\n",
      "image 125/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_19.jpg: 384x640 3\n",
      "image 126/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_20.jpg: 384x640 2\n",
      "image 127/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_21.jpg: 384x640 6\n",
      "image 128/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_22.jpg: 384x640 2\n",
      "image 129/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_23.jpg: 384x640 7\n",
      "image 130/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_24.jpg: 384x640 2\n",
      "image 131/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_25.jpg: 384x640 3\n",
      "image 132/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_26.jpg: 384x640 2\n",
      "image 133/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_27.jpg: 384x640 3\n",
      "image 134/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_28.jpg: 384x640 5\n",
      "image 135/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_29.jpg: 384x640 1\n",
      "image 136/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_30.jpg: 384x640 1\n",
      "image 137/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_31.jpg: 384x640 5\n",
      "image 138/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_32.jpg: 384x640 3\n",
      "image 139/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_33.jpg: 384x640 2\n",
      "image 140/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_34.jpg: 384x640 3\n",
      "image 141/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_35.jpg: 384x640 4\n",
      "image 142/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_36.jpg: 384x640 2\n",
      "image 143/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_37.jpg: 384x640 2\n",
      "image 144/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_38.jpg: 384x640 2\n",
      "image 145/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_39.jpg: 384x640 4\n",
      "image 146/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_40.jpg: 384x640 3\n",
      "image 147/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_41.jpg: 384x640 3\n",
      "image 148/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_42.jpg: 384x640 4\n",
      "image 149/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_43.jpg: 384x640 6\n",
      "image 150/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_44.jpg: 384x640 5\n",
      "image 151/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_45.jpg: 384x640 2\n",
      "image 152/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_46.jpg: 384x640 2\n",
      "image 153/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_47.jpg: 384x640 3\n",
      "Done. (12.644s)\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(sub)\n",
    "submission = submission.drop([0])\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    t0 = time.time()\n",
    "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "    _ = model(img) # run once\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        basename = os.path.basename(path)\n",
    "        basename_no_ext = os.path.splitext(basename)[0]\n",
    "        \n",
    "        h, w, _ = im0s.shape\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            p, s, im0 = path, '', im0s\n",
    "#             save_path = str(Path(out) / Path(p).name)\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if det is not None and len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += '%g' % n  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in det:\n",
    "                    crop_img = im0[int(xyxy[1]):int(xyxy[3]), int(xyxy[0]):int(xyxy[2])]\n",
    "                    crop_img_resize = cv2.resize(crop_img[:,:,::-1], (192, 384))\n",
    "                    crop_img_resize = ((crop_img_resize/255 - (0.485, 0.456, 0.406)) / (0.229, 0.224, 0.225)).astype('float32') # 0 - 255 to 0.0 - 1.0\n",
    "                    crop_img_resize = crop_img_resize.transpose((2, 0, 1))\n",
    "                    crop_img_resize = np.expand_dims(crop_img_resize, 0)\n",
    "                    crop_img_resize = torch.from_numpy(crop_img_resize).to(device)\n",
    "                    reid_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outputs, _ = reid_model(crop_img_resize)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                    reid = class_names[preds[0]]\n",
    "                    submission.loc[len(submission)] = [basename_no_ext,\n",
    "                                                       reid,\n",
    "                                                       float(conf.cpu()),\n",
    "                                                       int(xyxy[0].cpu())/w,\n",
    "                                                       int(xyxy[1].cpu())/h,\n",
    "                                                       int(xyxy[2].cpu())/w,\n",
    "                                                       int(xyxy[3].cpu())/h]\n",
    "            print(s)\n",
    "#                     plot_one_box(xyxy, im0, label=reid, color=colors[class_names.index(reid)], line_thickness=1)\n",
    "#             cv2.imwrite(save_path, im0)\n",
    "#     print('Results saved to %s' % Path(out))\n",
    "    print('Done. (%.3fs)' % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b6a8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_yolor_csp_cosine.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dea9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mean_average_precision.detection_map import DetectionMAP\n",
    "answer = pd.read_csv('answer.csv')\n",
    "submission = pd.read_csv('submission_yolor_csp_cosine.csv')\n",
    "\n",
    "filenames = np.unique(answer['filename'])\n",
    "class_names = [  1,   3,  15,  55,  61,  63,  74,  75,  84,  87,  88,  90,  98,\n",
    "       111, 112, 116, 117, 122, 123, 124, 125, 126, 132, 133, 135, 148,\n",
    "       155, 162, 163, 166, 173, 174, 175, 181, 182, 183, 184, 185, 188,\n",
    "       192, 193, 197, 205, 216, 221, 222, 225, 228, 230, 231, 232, 233,\n",
    "       235, 265, 273, 285, 296, 302, 338, 339, 341, 344, 359, 516, 517,\n",
    "       543, 544, 545, 607, 618, 619, 621, 622, 623, 624, 626, 652, 653,\n",
    "       655, 656, 659, 660, 662, 663, 664, 705, 712, 713, 714, 734, 735,\n",
    "       736, 754, 835, 857, 880, 881, 882, 891, 894]\n",
    "cls_name_dict = {k:v for v, k in enumerate(class_names)}\n",
    "submission['reid'] = submission['reid'].map(cls_name_dict)\n",
    "answer['reid'] = answer['reid'].map(cls_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5feeb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for file in filenames:\n",
    "    sub = submission[submission['filename']==file]\n",
    "    ans = answer[answer['filename']==file]\n",
    "    pred_bb = sub[['left', 'top', 'right', 'bottom']].values\n",
    "    pred_cls = sub['reid'].values\n",
    "    pred_conf = sub['confidence'].values\n",
    "    gt_bb = ans[['left', 'top', 'right', 'bottom']].values\n",
    "    gt_cls = ans['reid'].values\n",
    "    frames.append((pred_bb, pred_cls, pred_conf, gt_bb, gt_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14a0b4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@.5: 0.7247240183175458\n",
      "mAP@.55: 0.7247240183175458\n",
      "mAP@.6: 0.7227170253105527\n",
      "mAP@.65: 0.7187790633151195\n",
      "mAP@.7: 0.7109825166705125\n",
      "mAP@.75: 0.6945085880707758\n",
      "mAP@.8: 0.6373163321788664\n",
      "mAP@.85: 0.5254621956458038\n",
      "mAP@.9: 0.4026095139332551\n",
      "mAP@.95: 0.34186887387974346\n",
      "mAP@.5:.95: 0.6203692145639721\n"
     ]
    }
   ],
   "source": [
    "n_class = len(class_names)\n",
    "\n",
    "thresh = [0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95]\n",
    "mAP_all = []\n",
    "for thre in thresh:\n",
    "    mAP = DetectionMAP(n_class, overlap_threshold=thre)\n",
    "    for i, frame in enumerate(frames):\n",
    "    #     print(\"Evaluate frame {}\".format(i))\n",
    "    #     show_frame(*frame)\n",
    "        mAP.evaluate(*frame)\n",
    "    print('mAP@' + str(thre)[1:] + f': {mAP.compute_mAP()}')\n",
    "    mAP_all.append(mAP.compute_mAP())\n",
    "\n",
    "print(f'mAP@.5:.95: {np.average(mAP_all)}')\n",
    "# print('mAP:', mAP.compute_mAP())\n",
    "# mAP.plot(class_names=class_names, figsize=30)\n",
    "# plt.show()\n",
    "#plt.savefig(\"pr_curve_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130adf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOv4_detect.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
