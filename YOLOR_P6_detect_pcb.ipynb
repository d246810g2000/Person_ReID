{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e14199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4903f7",
   "metadata": {},
   "source": [
    "## 資料處理，從標記檔中取得行人影像並依照 reid 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c2ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagesInDir(dir_path):\n",
    "    img_formats = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng']  # acceptable image suffixes\n",
    "    image_list = []\n",
    "    for img_format in img_formats:\n",
    "        for filename in glob.glob(dir_path + f'/*.{img_format}'):\n",
    "            image_list.append(filename)\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e8106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['pedestrian']\n",
    "img_dir = 'person_reid_datasets/train/JPEGImages/'\n",
    "ann_dir = 'person_reid_datasets/train/Annotations/'\n",
    "image_paths = getImagesInDir(img_dir)\n",
    "out_dir = 'reid/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7150ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建資料夾\n",
    "if os.path.exists(out_dir.split('/')[0]):\n",
    "    shutil.rmtree(out_dir.split('/')[0])  # delete output folder\n",
    "os.makedirs(out_dir)  # make new output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7017fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 723/723 [00:08<00:00, 87.98it/s] \n"
     ]
    }
   ],
   "source": [
    "for img_path in tqdm(image_paths, total=len(image_paths)):\n",
    "    # read image\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    basename = os.path.basename(img_path)\n",
    "    basename_no_ext = os.path.splitext(basename)[0]\n",
    "\n",
    "    in_file = open(ann_dir + '/' + basename_no_ext + '.xml')\n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "    cnt=0\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in classes or int(difficult)==1:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
    "        reid = obj.find('reid').text[:-2]\n",
    "        crop_img = img[int(b[2]):int(b[3]), int(b[0]):int(b[1]), :]\n",
    "        out_reid_dir = os.path.join(out_dir, reid)\n",
    "        if not os.path.exists(out_reid_dir):\n",
    "            os.mkdir(out_reid_dir)\n",
    "        cv2.imwrite(os.path.join(out_reid_dir, basename_no_ext+'_'+str(cnt))+'.jpg', crop_img)\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5198512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6s2_058318_4.jpg</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c5s2_049580_3.jpg</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c5s2_070777_4.jpg</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c4s4_050685_1.jpg</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c6s2_058393_1.jpg</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                file class\n",
       "0  c6s2_058318_4.jpg   624\n",
       "1  c5s2_049580_3.jpg   624\n",
       "2  c5s2_070777_4.jpg   624\n",
       "3  c4s4_050685_1.jpg   624\n",
       "4  c6s2_058393_1.jpg   624"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = 'reid/data'\n",
    "x_data_list = []\n",
    "y_data_list = []\n",
    "for roots, _, files in os.walk(out_dir):\n",
    "    for each in files:\n",
    "        if each.find('checkpoint') == -1:\n",
    "            x_data_list.append(each)\n",
    "            y_data_list.append(roots.split(\"/\")[-1])\n",
    "data = pd.DataFrame({'file':x_data_list, 'class':y_data_list})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92819b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file\n",
       "class      \n",
       "285       1\n",
       "545       1\n",
       "714       1\n",
       "344       2\n",
       "618       2\n",
       "...     ...\n",
       "192      45\n",
       "55       46\n",
       "173      48\n",
       "155      57\n",
       "233      69\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('class').count().sort_values(['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7d8591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f10432cba50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.groupby('class').count().plot(kind='bar', figsize=(15, 5), legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef1495b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = len(data.groupby('class').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4b026",
   "metadata": {},
   "source": [
    "## 讀入 ReID Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c36b414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from model import PCB\n",
    "reid_model = PCB(751).to(device).eval()\n",
    "reid_model.load_state_dict(torch.load('pcb.pth') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb46cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    names = os.listdir(folder_path)\n",
    "    if '.ipynb_checkpoints' in names:\n",
    "        names.remove(\".ipynb_checkpoints\")\n",
    "    data = {}\n",
    "    total_person = 0\n",
    "    for roots, dirs, files in os.walk(folder_path):\n",
    "        if roots.find(\".ipynb_checkpoints\") == -1:\n",
    "            name = roots.split('/')[-1]\n",
    "            if name in names: \n",
    "                data[name] = []\n",
    "                for file in files:\n",
    "                    total_person += 1\n",
    "                    data[name].append(os.path.join(roots, file))\n",
    "    print('total person:', total_person)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227ec83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(extractor, img):\n",
    "    img = ((img/255 - (0.485, 0.456, 0.406)) / (0.229, 0.224, 0.225)).astype('float32')\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    inputs = np.expand_dims(img, axis=0)\n",
    "    inputs = torch.from_numpy(inputs).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = extractor(inputs)\n",
    "    outputs = normalize(embedding.cpu().numpy()).flatten()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bdb40d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total person: 1438\n"
     ]
    }
   ],
   "source": [
    "# load face data from database\n",
    "folder_path = 'reid/data'\n",
    "data = load_data(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79a88f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100it [00:16,  6.09it/s]\n"
     ]
    }
   ],
   "source": [
    "n = len(data)\n",
    "allembeddings = np.zeros([len(data), 256])\n",
    "for j, name in tqdm(enumerate(data.keys())):\n",
    "    embeddings = np.zeros([len(data[name]), 256])\n",
    "    for i, file in enumerate(data[name]):\n",
    "        img = cv2.imread(file, cv2.IMREAD_COLOR).astype('float32')\n",
    "        img = cv2.resize(img[:,:,::-1], (192, 384))\n",
    "        # extract persons and calculate person embeddings for the photo files\n",
    "        embeddings[i] = get_embeddings(reid_model, img)\n",
    "    # calculate mean of the embeddings\n",
    "    allembeddings[j] = embeddings.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbc1a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {'names': list(data.keys()), 'embeddings': allembeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "412f4365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allembeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f884a5f",
   "metadata": {
    "id": "UL_9sBkuYsBz"
   },
   "source": [
    "# YOLOR + ReID 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "421b62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('yolor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34605fce",
   "metadata": {
    "id": "IeXiY6goZNjE"
   },
   "source": [
    "## 匯入所需套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01296f10",
   "metadata": {
    "id": "a91de30c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.google_utils import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import (\n",
    "    check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, strip_optimizer)\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "\n",
    "from models.models import *\n",
    "from utils.datasets import *\n",
    "from utils.general import *\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0859bb1f",
   "metadata": {
    "id": "811cd6d5"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84f989",
   "metadata": {
    "id": "89uLk_rVZsh3"
   },
   "source": [
    "## 參數設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f55a55e",
   "metadata": {
    "id": "5a4cafe9"
   },
   "outputs": [],
   "source": [
    "weights = 'weights/yolor_p6.pt'\n",
    "source = 'person_reid_datasets/test/'\n",
    "sub = 'person_reid_datasets/sample_submission.csv'\n",
    "cfg = 'cfg/yolor_p6_pedestrian.cfg'\n",
    "imgsz = 1280\n",
    "conf_thres = 0.25\n",
    "iou_thres = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7e282",
   "metadata": {
    "id": "ULXI0iSRZaJD"
   },
   "source": [
    "## 將訓練好的偵測模型權重和 ReID 模型權重讀入到各自的模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab8824e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26c6a5ae",
    "outputId": "134ef9a3-30f9-465f-c1d5-df0411900942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Darknet(cfg, imgsz).to(device).eval()\n",
    "model.load_state_dict(torch.load(weights, map_location=device)['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d62c7f",
   "metadata": {
    "id": "l1kAub6baCpD"
   },
   "source": [
    "## 預測影像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6fdfecf",
   "metadata": {
    "id": "JgM71kIdaNl_"
   },
   "outputs": [],
   "source": [
    "dataset = LoadImages(source, img_size=imgsz, auto_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba053f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_persons(db, embedding, threshold):\n",
    "    # read db\n",
    "    db_embeddings = db['embeddings']\n",
    "    db_names = db['names']\n",
    "\n",
    "    distances = np.zeros((len(db_embeddings)))\n",
    "    for i, db_embedding in enumerate(db_embeddings):\n",
    "        distance = round(np.linalg.norm(db_embedding-embedding), 1)\n",
    "        distances[i] = distance\n",
    "    idx_min = np.argmin(distances)\n",
    "    distance, name = distances[idx_min], db_names[idx_min]\n",
    "    if distance < threshold:\n",
    "        return name, distance\n",
    "    else:\n",
    "        return False, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93f53654",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05d01f15",
    "outputId": "d18393dc-9c08-465e-8b20-ba1ee724bfcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_01.jpg: 768x1280 4\n",
      "image 2/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_02.jpg: 768x1280 2\n",
      "image 3/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_03.jpg: 768x1280 3\n",
      "image 4/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_04.jpg: 768x1280 4\n",
      "image 5/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_05.jpg: 768x1280 2\n",
      "image 6/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_06.jpg: 768x1280 4\n",
      "image 7/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_07.jpg: 768x1280 4\n",
      "image 8/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_08.jpg: 768x1280 4\n",
      "image 9/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_09.jpg: 768x1280 7\n",
      "image 10/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_10.jpg: 768x1280 3\n",
      "image 11/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_11.jpg: 768x1280 4\n",
      "image 12/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_12.jpg: 768x1280 3\n",
      "image 13/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_13.jpg: 768x1280 2\n",
      "image 14/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_14.jpg: 768x1280 1\n",
      "image 15/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_15.jpg: 768x1280 3\n",
      "image 16/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_16.jpg: 768x1280 1\n",
      "image 17/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_17.jpg: 768x1280 3\n",
      "image 18/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_18.jpg: 768x1280 4\n",
      "image 19/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_19.jpg: 768x1280 2\n",
      "image 20/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_20.jpg: 768x1280 3\n",
      "image 21/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_21.jpg: 768x1280 4\n",
      "image 22/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_22.jpg: 768x1280 1\n",
      "image 23/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_23.jpg: 768x1280 3\n",
      "image 24/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_24.jpg: 768x1280 1\n",
      "image 25/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_25.jpg: 768x1280 1\n",
      "image 26/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_26.jpg: 768x1280 2\n",
      "image 27/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_27.jpg: 768x1280 2\n",
      "image 28/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_28.jpg: 768x1280 4\n",
      "image 29/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_29.jpg: 768x1280 1\n",
      "image 30/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_30.jpg: 768x1280 3\n",
      "image 31/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_31.jpg: 768x1280 1\n",
      "image 32/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_32.jpg: 768x1280 1\n",
      "image 33/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_33.jpg: 768x1280 3\n",
      "image 34/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_34.jpg: 768x1280 4\n",
      "image 35/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_35.jpg: 768x1280 3\n",
      "image 36/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_36.jpg: 768x1280 2\n",
      "image 37/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_37.jpg: 768x1280 2\n",
      "image 38/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_38.jpg: 768x1280 1\n",
      "image 39/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_39.jpg: 768x1280 4\n",
      "image 40/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_40.jpg: 768x1280 2\n",
      "image 41/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_41.jpg: 768x1280 1\n",
      "image 42/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_42.jpg: 768x1280 6\n",
      "image 43/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_43.jpg: 768x1280 3\n",
      "image 44/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_44.jpg: 768x1280 5\n",
      "image 45/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_45.jpg: 768x1280 2\n",
      "image 46/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_46.jpg: 768x1280 1\n",
      "image 47/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_47.jpg: 768x1280 7\n",
      "image 48/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_48.jpg: 768x1280 4\n",
      "image 49/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_49.jpg: 768x1280 5\n",
      "image 50/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_50.jpg: 768x1280 1\n",
      "image 51/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_51.jpg: 768x1280 2\n",
      "image 52/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_52.jpg: 768x1280 2\n",
      "image 53/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_53.jpg: 768x1280 1\n",
      "image 54/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_54.jpg: 768x1280 3\n",
      "image 55/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_55.jpg: 768x1280 1\n",
      "image 56/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_56.jpg: 768x1280 1\n",
      "image 57/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_57.jpg: 768x1280 1\n",
      "image 58/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_58.jpg: 768x1280 3\n",
      "image 59/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_59.jpg: 768x1280 2\n",
      "image 60/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_60.jpg: 768x1280 8\n",
      "image 61/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_61.jpg: 768x1280 1\n",
      "image 62/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_62.jpg: 768x1280 4\n",
      "image 63/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_63.jpg: 768x1280 2\n",
      "image 64/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_64.jpg: 768x1280 3\n",
      "image 65/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_65.jpg: 768x1280 3\n",
      "image 66/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_66.jpg: 768x1280 2\n",
      "image 67/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_67.jpg: 768x1280 3\n",
      "image 68/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_68.jpg: 768x1280 1\n",
      "image 69/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_69.jpg: 768x1280 3\n",
      "image 70/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_70.jpg: 768x1280 4\n",
      "image 71/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_71.jpg: 768x1280 2\n",
      "image 72/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c1_72.jpg: 768x1280 4\n",
      "image 73/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_01.jpg: 768x1280 3\n",
      "image 74/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_02.jpg: 768x1280 2\n",
      "image 75/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_03.jpg: 768x1280 2\n",
      "image 76/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_04.jpg: 768x1280 3\n",
      "image 77/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_05.jpg: 768x1280 1\n",
      "image 78/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_06.jpg: 768x1280 3\n",
      "image 79/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_07.jpg: 768x1280 2\n",
      "image 80/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_08.jpg: 768x1280 2\n",
      "image 81/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_09.jpg: 768x1280 2\n",
      "image 82/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_10.jpg: 768x1280 4\n",
      "image 83/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_11.jpg: 768x1280 1\n",
      "image 84/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_12.jpg: 768x1280 2\n",
      "image 85/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_13.jpg: 768x1280 1\n",
      "image 86/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_14.jpg: 768x1280 2\n",
      "image 87/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_15.jpg: 768x1280 1\n",
      "image 88/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_16.jpg: 768x1280 1\n",
      "image 89/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_17.jpg: 768x1280 3\n",
      "image 90/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_18.jpg: 768x1280 4\n",
      "image 91/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_19.jpg: 768x1280 1\n",
      "image 92/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_20.jpg: 768x1280 1\n",
      "image 93/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_21.jpg: 768x1280 2\n",
      "image 94/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_22.jpg: 768x1280 2\n",
      "image 95/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_23.jpg: 768x1280 2\n",
      "image 96/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_24.jpg: 768x1280 3\n",
      "image 97/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_25.jpg: 768x1280 2\n",
      "image 98/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_26.jpg: 768x1280 1\n",
      "image 99/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_27.jpg: 768x1280 2\n",
      "image 100/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_28.jpg: 768x1280 1\n",
      "image 101/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_29.jpg: 768x1280 1\n",
      "image 102/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_30.jpg: 768x1280 2\n",
      "image 103/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_31.jpg: 768x1280 4\n",
      "image 104/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_32.jpg: 768x1280 2\n",
      "image 105/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_33.jpg: 768x1280 1\n",
      "image 106/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c2_34.jpg: 768x1280 5\n",
      "image 107/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_01.jpg: 768x1280 3\n",
      "image 108/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_02.jpg: 768x1280 5\n",
      "image 109/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_03.jpg: 768x1280 1\n",
      "image 110/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_04.jpg: 768x1280 2\n",
      "image 111/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_05.jpg: 768x1280 3\n",
      "image 112/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_06.jpg: 768x1280 3\n",
      "image 113/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_07.jpg: 768x1280 4\n",
      "image 114/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_08.jpg: 768x1280 3\n",
      "image 115/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_09.jpg: 768x1280 3\n",
      "image 116/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_10.jpg: 768x1280 3\n",
      "image 117/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_11.jpg: 768x1280 2\n",
      "image 118/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_12.jpg: 768x1280 5\n",
      "image 119/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_13.jpg: 768x1280 4\n",
      "image 120/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_14.jpg: 768x1280 2\n",
      "image 121/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_15.jpg: 768x1280 3\n",
      "image 122/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_16.jpg: 768x1280 3\n",
      "image 123/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_17.jpg: 768x1280 4\n",
      "image 124/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_18.jpg: 768x1280 3\n",
      "image 125/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_19.jpg: 768x1280 2\n",
      "image 126/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_20.jpg: 768x1280 2\n",
      "image 127/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_21.jpg: 768x1280 6\n",
      "image 128/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_22.jpg: 768x1280 2\n",
      "image 129/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_23.jpg: 768x1280 7\n",
      "image 130/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_24.jpg: 768x1280 3\n",
      "image 131/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_25.jpg: 768x1280 3\n",
      "image 132/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_26.jpg: 768x1280 2\n",
      "image 133/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_27.jpg: 768x1280 3\n",
      "image 134/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_28.jpg: 768x1280 5\n",
      "image 135/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_29.jpg: 768x1280 2\n",
      "image 136/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_30.jpg: 768x1280 1\n",
      "image 137/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_31.jpg: 768x1280 5\n",
      "image 138/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_32.jpg: 768x1280 2\n",
      "image 139/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_33.jpg: 768x1280 2\n",
      "image 140/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_34.jpg: 768x1280 3\n",
      "image 141/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_35.jpg: 768x1280 2\n",
      "image 142/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_36.jpg: 768x1280 2\n",
      "image 143/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_37.jpg: 768x1280 2\n",
      "image 144/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_38.jpg: 768x1280 2\n",
      "image 145/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_39.jpg: 768x1280 4\n",
      "image 146/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_40.jpg: 768x1280 2\n",
      "image 147/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_41.jpg: 768x1280 3\n",
      "image 148/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_42.jpg: 768x1280 4\n",
      "image 149/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_43.jpg: 768x1280 5\n",
      "image 150/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_44.jpg: 768x1280 4\n",
      "image 151/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_45.jpg: 768x1280 2\n",
      "image 152/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_46.jpg: 768x1280 2\n",
      "image 153/153 /home/jovyan/Person_ReID/person_reid_datasets/test/c3_47.jpg: 768x1280 3\n",
      "Done. (15.185s)\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(sub)\n",
    "submission = submission.drop([0])\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    t0 = time.time()\n",
    "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "    _ = model(img) # run once\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        basename = os.path.basename(path)\n",
    "        basename_no_ext = os.path.splitext(basename)[0]\n",
    "        \n",
    "        h, w, _ = im0s.shape\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            p, s, im0 = path, '', im0s\n",
    "#             save_path = str(Path(out) / Path(p).name)\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if det is not None and len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += '%g' % n  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in det:\n",
    "                    crop_img = im0[int(xyxy[1]):int(xyxy[3]), int(xyxy[0]):int(xyxy[2])]\n",
    "                    crop_img_resize = cv2.resize(crop_img[:,:,::-1], (192, 384))\n",
    "                    crop_img_resize = ((crop_img_resize/255 - (0.485, 0.456, 0.406)) / (0.229, 0.224, 0.225)).astype('float32') # 0 - 255 to 0.0 - 1.0\n",
    "                    crop_img_resize = crop_img_resize.transpose((2, 0, 1))\n",
    "                    crop_img_resize = np.expand_dims(crop_img_resize, 0)\n",
    "                    crop_img_resize = torch.from_numpy(crop_img_resize).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        preds = reid_model(crop_img_resize)\n",
    "                        embedding = normalize(preds.cpu())\n",
    "                        matches = compare_persons(db, embedding, 2)\n",
    "                        distance = matches[1]\n",
    "                        if False not in matches:\n",
    "                            reid = matches[0]\n",
    "                    submission.loc[len(submission)] = [basename_no_ext,\n",
    "                                                       reid,\n",
    "                                                       float(conf.cpu()),\n",
    "                                                       int(xyxy[0].cpu())/w,\n",
    "                                                       int(xyxy[1].cpu())/h,\n",
    "                                                       int(xyxy[2].cpu())/w,\n",
    "                                                       int(xyxy[3].cpu())/h]\n",
    "            print(s)\n",
    "#                     plot_one_box(xyxy, im0, label=reid, color=colors[class_names.index(reid)], line_thickness=1)\n",
    "#             cv2.imwrite(save_path, im0)\n",
    "#     print('Results saved to %s' % Path(out))\n",
    "    print('Done. (%.3fs)' % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28d0cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_yolor_p6_pcb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9072b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mean_average_precision.detection_map import DetectionMAP\n",
    "answer = pd.read_csv('answer.csv')\n",
    "submission = pd.read_csv('submission_yolor_p6_pcb.csv')\n",
    "\n",
    "filenames = np.unique(answer['filename'])\n",
    "class_names = [  1,   3,  15,  55,  61,  63,  74,  75,  84,  87,  88,  90,  98,\n",
    "       111, 112, 116, 117, 122, 123, 124, 125, 126, 132, 133, 135, 148,\n",
    "       155, 162, 163, 166, 173, 174, 175, 181, 182, 183, 184, 185, 188,\n",
    "       192, 193, 197, 205, 216, 221, 222, 225, 228, 230, 231, 232, 233,\n",
    "       235, 265, 273, 285, 296, 302, 338, 339, 341, 344, 359, 516, 517,\n",
    "       543, 544, 545, 607, 618, 619, 621, 622, 623, 624, 626, 652, 653,\n",
    "       655, 656, 659, 660, 662, 663, 664, 705, 712, 713, 714, 734, 735,\n",
    "       736, 754, 835, 857, 880, 881, 882, 891, 894]\n",
    "cls_name_dict = {k:v for v, k in enumerate(class_names)}\n",
    "submission['reid'] = submission['reid'].map(cls_name_dict)\n",
    "answer['reid'] = answer['reid'].map(cls_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de091f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for file in filenames:\n",
    "    sub = submission[submission['filename']==file]\n",
    "    ans = answer[answer['filename']==file]\n",
    "    pred_bb = sub[['left', 'top', 'right', 'bottom']].values\n",
    "    pred_cls = sub['reid'].values\n",
    "    pred_conf = sub['confidence'].values\n",
    "    gt_bb = ans[['left', 'top', 'right', 'bottom']].values\n",
    "    gt_cls = ans['reid'].values\n",
    "    frames.append((pred_bb, pred_cls, pred_conf, gt_bb, gt_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8e412e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@.5: 0.875156265085193\n",
      "mAP@.55: 0.8737276936566214\n",
      "mAP@.6: 0.8723501426362132\n",
      "mAP@.65: 0.8673703446564153\n",
      "mAP@.7: 0.8607702202437909\n",
      "mAP@.75: 0.8212202106711634\n",
      "mAP@.8: 0.7669317179625168\n",
      "mAP@.85: 0.6422264198357572\n",
      "mAP@.9: 0.47855256551570524\n",
      "mAP@.95: 0.3585903867260264\n",
      "mAP@.5:.95: 0.7416895966989403\n"
     ]
    }
   ],
   "source": [
    "n_class = len(class_names)\n",
    "\n",
    "thresh = [0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95]\n",
    "mAP_all = []\n",
    "for thre in thresh:\n",
    "    mAP = DetectionMAP(n_class, overlap_threshold=thre)\n",
    "    for i, frame in enumerate(frames):\n",
    "    #     print(\"Evaluate frame {}\".format(i))\n",
    "    #     show_frame(*frame)\n",
    "        mAP.evaluate(*frame)\n",
    "    print('mAP@' + str(thre)[1:] + f': {mAP.compute_mAP()}')\n",
    "    mAP_all.append(mAP.compute_mAP())\n",
    "\n",
    "print(f'mAP@.5:.95: {np.average(mAP_all)}')\n",
    "# print('mAP:', mAP.compute_mAP())\n",
    "# mAP.plot(class_names=class_names, figsize=30)\n",
    "# plt.show()\n",
    "#plt.savefig(\"pr_curve_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf732a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOv4_detect.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
